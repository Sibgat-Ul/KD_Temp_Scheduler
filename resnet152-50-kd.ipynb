{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":998277,"sourceType":"datasetVersion","datasetId":547506}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport tqdm\n\n# Define temperature and alpha for distillation\nTEMPERATURE = 3.0\nALPHA = 0.9\n\nweights101 = models.ResNet152_Weights.DEFAULT\ntransforms = weights101.transforms()\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load Teacher and Student Models\nteacher_model = models.resnet152(\n    weights=weights101\n).to(device)\n\nprint(\"======TEACHER======\")\nprint(teacher_model)\n\nstudent_model = models.resnet50(weights=None).to(device)\n\nprint(\"======STUDENT======\")\nprint(student_model)\n\n# Freeze teacher model parameters\nfor param in teacher_model.parameters():\n    param.requires_grad = False\n\n# Define loss functions\ncriterion = nn.CrossEntropyLoss()\ndistillation_loss = nn.KLDivLoss(reduction='batchmean')\n\n# Define optimizer for the student model\noptimizer = optim.Adam(student_model.parameters(), lr=0.001)\n\n# Transform and load your dataset\ntrain_dataset = datasets.ImageFolder(\n    '/kaggle/input/imagenetmini-1000/imagenet-mini/train', \n    transform=transforms\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the top-k accuracy for the specified values of k.\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n# Knowledge Distillation Function\ndef distillation_loss_fn(student_logits, teacher_logits, labels):\n    # Regular cross-entropy loss\n    loss = criterion(student_logits, labels)\n    \n    # Softened outputs for distillation\n    distillation_loss_value = distillation_loss(\n        torch.nn.functional.log_softmax(student_logits / TEMPERATURE, dim=1),\n        torch.nn.functional.softmax(teacher_logits / TEMPERATURE, dim=1)\n    ) * (TEMPERATURE ** 2)\n    \n    # Weighted sum of both losses\n    return ALPHA * distillation_loss_value + (1 - ALPHA) * loss\n\n# Training Loop\ndef train_student(teacher_model, student_model, dataloader, optimizer, epochs=10):\n    student_model.train()\n    teacher_model.eval()\n    \n    best_loss = float('inf')\n    best_acc = 0.0\n    \n    history = {\n        'loss': [],\n        'acc1': [],\n        'acc5': []\n    }\n    \n    for epoch in range(epochs):\n        epoch_loss = 0.0\n        epoch_acc1 = 0.0\n        epoch_acc5 = 0.0\n        pbar = tqdm.tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}')\n        \n        for i, (images, labels) in enumerate(pbar):\n            inputs, labels = images.to(device), labels.to(device)\n            \n            # Get teacher predictions (detach to avoid computing gradients on teacher)\n            with torch.no_grad():\n                teacher_logits = teacher_model(inputs)\n            \n            # Get student predictions\n            student_logits = student_model(inputs)\n            \n            # Compute loss\n            loss = distillation_loss_fn(student_logits, teacher_logits, labels)\n            \n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            acc1, acc5 = accuracy(student_logits, labels, (1, 5))\n            acc1 = acc1.detach().cpu()\n            acc5 = acc5.detach().cpu()\n            \n            # Update running metrics\n            batch_size = images.size(0)\n            epoch_loss += loss.item() * batch_size\n            epoch_acc1 += acc1 * batch_size\n            epoch_acc5 += acc5 * batch_size\n\n            # Update progress bar\n            pbar.set_postfix({\n                'loss': f'{loss.item():.4f}',\n                'acc1': f'{acc1.item():.2f}%',\n                'acc5': f'{acc5.item():.2f}%'\n            })\n            \n        # Calculate epoch metrics\n        num_samples = len(train_loader)\n        epoch_loss /= num_samples\n        epoch_acc1 /= num_samples\n        epoch_acc5 /= num_samples\n        \n        # Update history\n        history['loss'].append(epoch_loss)\n        history['acc1'].append(epoch_acc1)\n        history['acc5'].append(epoch_acc5)\n\n        # Print epoch summary\n        print(f'\\nEpoch {epoch+1}/{max_epoch}:')\n        print(f'Loss: {epoch_loss:.4f}')\n        print(f'Accuracy@1: {epoch_acc1:.2f}%')\n        print(f'Accuracy@5: {epoch_acc5:.2f}%')\n        \n        # Save best model\n        if epoch_loss < best_loss:\n            best_loss = epoch_loss\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': student_model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': best_loss,\n                'accuracy': epoch_acc1\n            }, 'best_model.pth')\n        \n    return student_model\n\n# Train the student model\ntrained_student_model = train_student(teacher_model, student_model, train_loader, optimizer, epochs=100)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-28T14:54:48.991923Z","iopub.execute_input":"2024-10-28T14:54:48.992312Z","iopub.status.idle":"2024-10-28T15:00:44.791311Z","shell.execute_reply.started":"2024-10-28T14:54:48.992277Z","shell.execute_reply":"2024-10-28T15:00:44.789757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torchvision import models\nimport torch\n\n# Function to evaluate model accuracy\ndef evaluate_model(model, dataloader, device):\n    model.eval()\n    correct_top1 = 0\n    correct_top5 = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            \n            # Top-1 accuracy\n            _, predicted = outputs.max(1)\n            correct_top1 += (predicted == labels).sum().item()\n            \n            # Top-5 accuracy\n            _, top5_pred = outputs.topk(5, dim=1)\n            correct_top5 += (top5_pred == labels.view(-1, 1)).sum().item()\n            \n            total += labels.size(0)\n    \n    top1_accuracy = 100 * correct_top1 / total\n    top5_accuracy = 100 * correct_top5 / total\n    return top1_accuracy, top5_accuracy\n\n# DataLoader for the validation set (adjust the path accordingly)\nval_dataset = datasets.ImageFolder('/kaggle/input/imagenetmini-1000/imagenet-mini/val', transform=transform)\nval_loader = DataLoader(val_dataset, batch_size=40, shuffle=False)\n\ntop1_teacher, top5_teacher = evaluate_model(teacher_model, val_loader, device)\nprint(f\"Teacher Model - Top-1 Accuracy: {top1_teacher:.2f}%, Top-5 Accuracy: {top5_teacher:.2f}%\")\n\n# Train student_model here without distillation, then evaluate\nstudent_model_wokd = models.resnet50(pretrained=True).to(device)\ntop1_student_baseline, top5_student_baseline = evaluate_model(student_model_wokd, val_loader, device)\nprint(f\"Student Model Baseline - Top-1 Accuracy: {top1_student_baseline:.2f}%, Top-5 Accuracy: {top5_student_baseline:.2f}%\")\n\n# Train the student model with knowledge distillation and evaluate\n# (Use the previously provided code for distillation training)\ntop1_student_distilled, top5_student_distilled = evaluate_model(trained_student_model, val_loader, device)\nprint(f\"Student Model with Distillation - Top-1 Accuracy: {top1_student_distilled:.2f}%, Top-5 Accuracy: {top5_student_distilled:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:35:38.785909Z","iopub.status.idle":"2024-10-28T14:35:38.786291Z","shell.execute_reply.started":"2024-10-28T14:35:38.786112Z","shell.execute_reply":"2024-10-28T14:35:38.78613Z"},"trusted":true},"execution_count":null,"outputs":[]}]}