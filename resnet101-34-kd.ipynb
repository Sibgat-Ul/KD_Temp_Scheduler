{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":998277,"sourceType":"datasetVersion","datasetId":547506}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"gpuType":"T4","name":"resnet101-34-kd"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n# THEN FEEL FREE TO DELETE THIS CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\n# import kagglehub\n# ifigotin_imagenetmini_1000_path = kagglehub.dataset_download('ifigotin/imagenetmini-1000')\n\n# print('Data source import complete.')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fn1OU_S9PFU6","outputId":"f4c7cb9d-1068-4a77-bf18-e8c0d9f275e8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"Downloading from https://www.kaggle.com/api/v1/datasets/download/ifigotin/imagenetmini-1000?dataset_version_number=1...\n"},{"output_type":"stream","name":"stderr","text":"100%|██████████| 3.92G/3.92G [02:21<00:00, 29.8MB/s]"},{"output_type":"stream","name":"stdout","text":"Extracting files...\n"},{"output_type":"stream","name":"stderr","text":"\n"},{"output_type":"stream","name":"stdout","text":"Data source import complete.\n"}]},{"cell_type":"code","source":"# !ls -a /root/.cache/kagglehub/datasets/ifigotin/imagenetmini-1000/versions/1/imagenet-mini","metadata":{"id":"jnWkUgxEedNB"},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\n\n# best_model_path = \"/root/.cache/kagglehub/models/sibgatulislam/distilled_resnet101/PyTorch/default/1/best_model.pth\"\ndataset_path = \"/kaggle/input/imagenetmini-1000/imagenet-mini\"","metadata":{"id":"auMdFU77fE0G"},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\n\nclass KnowledgeDistillationLoss(nn.Module):\n    def __init__(self, temperature=3.0, alpha=0.85):\n        \"\"\"\n        Custom loss for Knowledge Distillation\n\n        Args:\n            temperature (float): Temperature for softening logits\n            alpha (float): Balance between soft and hard targets\n        \"\"\"\n        super().__init__()\n        self.temperature = temperature\n        self.alpha = alpha\n        self.ce_loss = nn.CrossEntropyLoss()\n        self.kl_div_loss = nn.KLDivLoss(reduction='batchmean')\n\n    def forward(self, student_logits, teacher_logits, labels):\n        \"\"\"\n        Compute knowledge distillation loss\n\n        Args:\n            student_logits (torch.Tensor): Logits from student model\n            teacher_logits (torch.Tensor): Logits from teacher model\n            labels (torch.Tensor): Ground truth labels\n\n        Returns:\n            torch.Tensor: Combined loss\n        \"\"\"\n        # Soft targets with temperature scaling\n        soft_target_loss = self.kl_div_loss(\n            torch.log_softmax(student_logits / self.temperature, dim=1),\n            torch.softmax(teacher_logits / self.temperature, dim=1)\n        ) * (self.temperature ** 2)\n\n        # Hard target loss\n        hard_target_loss = self.ce_loss(student_logits, labels)\n\n        # Combined loss\n        return self.alpha * soft_target_loss + (1 - self.alpha) * hard_target_loss\n\ndef create_model(num_classes, is_teacher=False):\n    \"\"\"\n    Create ResNet model (teacher or student)\n\n    Args:\n        num_classes (int): Number of output classes\n        is_teacher (bool): Whether to create teacher or student model\n\n    Returns:\n        nn.Module: Configured ResNet model\n    \"\"\"\n    if is_teacher:\n        model = models.resnet101(weights=\"IMAGENET1K_V2\")\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n    else:\n        model = models.resnet34(weights=None)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n\n    return model\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"\n    Computes the top-k accuracy for the specified values of k\n\n    Args:\n        output (torch.Tensor): Model predictions\n        target (torch.Tensor): Ground truth labels\n        topk (tuple): Values of k for top-k accuracy\n\n    Returns:\n        list: List of top-k accuracies\n    \"\"\"\n\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\ndef train_knowledge_distillation(train_loader, test_loader, num_classes, device='cuda'):\n    \"\"\"\n    Perform Knowledge Distillation training\n\n    Args:\n        train_loader (DataLoader): Training data loader\n        test_loader (DataLoader): Validation data loader\n        num_classes (int): Number of output classes\n        device (str): Computing device\n    \"\"\"\n    # Initialize teacher and student models\n    teacher_model = create_model(num_classes, is_teacher=True).to(device)\n    student_model = create_model(num_classes, is_teacher=False).to(device)\n\n    # Freeze teacher model parameters\n    for param in teacher_model.parameters():\n        param.requires_grad = False\n\n    # Set up optimizer and loss\n    # optimizer = optim.Adam(student_model.parameters(), lr=1e-4)\n    optimizer = optim.SGD(student_model.parameters(), lr=0.001, momentum=0.9, nesterov=False, weight_decay=2e-05)\n    kd_loss_fn = KnowledgeDistillationLoss()\n\n    # Training loop\n    num_epochs = 100\n    best_accuracy = 0.0\n\n    for epoch in range(num_epochs):\n        student_model.train()\n        teacher_model.eval()\n        total_loss = 0.0\n        i = 0\n\n        for i, (inputs, labels) in enumerate(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # Zero gradients\n            optimizer.zero_grad()\n\n            # Forward pass\n            with torch.no_grad():\n                teacher_logits = teacher_model(inputs)\n\n            student_logits = student_model(inputs)\n\n            # Compute loss\n            loss = kd_loss_fn(student_logits, teacher_logits, labels)\n\n            acc1, acc5 = accuracy(student_logits, labels, (1, 5))\n\n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n\n            if i%50 == 0:\n              print(f\"epoch: {epoch} || batch: {i}, loss: {loss.item()}, acc@1: {acc1.item()}, acc@5: {acc5.item()}\")\n\n            total_loss += loss.item()\n\n        # Validation\n        student_model.eval()\n        correct = 0\n        total = 0\n\n        if (epoch) % 10 == 0:\n          with torch.no_grad():\n            for inputs, labels in test_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = student_model(inputs)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n                t_outputs = teacher_model(inputs)\n                _, t_predicted = torch.max(t_outputs.data, 1)\n                t_correct += (t_predicted == labels).sum().item()\n\n            v_accuracy = 100 * correct / total\n            t_accuracy = 100 * t_correct / total\n\n            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {v_accuracy:.2f}%, Teacher Accuracy: {t_accuracy:.2f}%')\n\n            # Save best model\n            if v_accuracy > best_accuracy:\n                best_accuracy = v_accuracy\n                torch.save(student_model.state_dict(), 'best_student_model.pth')\n                \n                \ndef main():\n    weights101 = models.ResNet101_Weights.IMAGENET1K_V2\n    transform = weights101.transforms()\n\n    # Data transformations\n    # transform = transforms.Compose([\n    #     transforms.Resize((224, 224)),\n    #     transforms.ToTensor(),\n    #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    # ])\n\n    # Load dataset (replace with your Kaggle dataset path)\n    train_dataset = ImageFolder(\n        dataset_path + '/train',\n        transform=transform\n    )\n\n    test_dataset = ImageFolder(\n      dataset_path + '/val',\n      transform=transform\n    )\n\n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n\n    # Number of classes in your dataset\n    num_classes = len(train_dataset.classes)\n\n    # Perform knowledge distillation\n    train_knowledge_distillation(train_loader, test_loader, num_classes)","metadata":{"id":"XN6uwibwYfOU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"id":"-zzYdB7zl-xz"},"execution_count":null,"outputs":[]}]}