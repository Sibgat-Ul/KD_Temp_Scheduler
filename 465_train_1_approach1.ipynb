{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 998277,
          "sourceType": "datasetVersion",
          "datasetId": 547506
        }
      ],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "465_train-1_approach1",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sibgat-Ul/KD_Temp_Scheduler/blob/main/465_train_1_approach1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "ifigotin_imagenetmini_1000_path = kagglehub.dataset_download('ifigotin/imagenetmini-1000')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "uWKoZ7OVV10C",
        "outputId": "e7fae76b-a797-466d-d42c-29b4707f77d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -a /root/.cache/kagglehub/datasets/ifigotin/imagenetmini-1000/versions/1/imagenet-mini"
      ],
      "metadata": {
        "id": "ga6LMtQbJguP",
        "outputId": "1b24e1e7-98b9-4f5c-b1d2-58a584283b30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".  ..  train  val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import os\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from tqdm import tqdm\n",
        "from torchvision.io import read_image, ImageReadMode\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_default_device('cuda')\n",
        "\n",
        "PATH = \"/kaggle/input/imagenetmini-1000/imagenet-mini\""
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-10-28T15:28:19.141745Z",
          "iopub.execute_input": "2024-10-28T15:28:19.142761Z",
          "iopub.status.idle": "2024-10-28T15:28:19.14909Z",
          "shell.execute_reply.started": "2024-10-28T15:28:19.14272Z",
          "shell.execute_reply": "2024-10-28T15:28:19.148226Z"
        },
        "trusted": true,
        "id": "8VVYrEZaV10E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "lwNOf_GHJpZO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get weights\n",
        "weights101 = models.ResNet101_Weights.DEFAULT\n",
        "transforms = weights101.transforms()\n",
        "\n",
        "dataset_path = \"/root/.cache/kagglehub/datasets/ifigotin/imagenetmini-1000/versions/1/imagenet-mini\"\n",
        "\n",
        "# Transform and load your dataset\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    dataset_path + '/train',\n",
        "    transform=transforms\n",
        ")\n",
        "\n",
        "# DataLoader for the validation set (adjust the path accordingly)\n",
        "val_dataset = datasets.ImageFolder(\n",
        "    dataset_path + '/val',\n",
        "    transform=transforms\n",
        ")\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "JOe31pIvJksb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "f3N_lnz6QZnr",
        "outputId": "254fb9f5-d23e-45c1-950c-0a7c2dbcdc4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define temperature and alpha for distillation\n",
        "TEMPERATURE = 3.0\n",
        "ALPHA = 0.95\n",
        "\n",
        "# Load Teacher and Student Models\n",
        "teacher_model = models.resnet101(\n",
        "    weights=weights101\n",
        ").to(device)\n",
        "\n",
        "# print(\"======TEACHER======\")\n",
        "# print(teacher_model)\n",
        "\n",
        "# student_model = models.resnet34(weights=None).to(device)\n",
        "\n",
        "# print(\"======STUDENT======\")\n",
        "# print(student_model)\n",
        "\n",
        "# Freeze teacher model parameters\n",
        "# for param in teacher_model.parameters():\n",
        "    # param.requires_grad = False\n",
        "\n",
        "# Define loss functions\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# distillation_loss = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "# Define optimizer for the student model\n",
        "# optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
        "# optimizer = optim.SGD(student_model.parameters(), lr=0.1, momentum=0.9, nesterov=True)\n"
      ],
      "metadata": {
        "id": "A1J80yCKJzAB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Approach-2"
      ],
      "metadata": {
        "id": "Phatxo4y_uJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torchvision.models as models\n",
        "from torchvision.models.resnet import ResNet, BasicBlock, Bottleneck\n",
        "\n",
        "class ResnetDT(ResNet):\n",
        "    def __init__(self, block_type=\"BasicBlock\", init_xavier=False, *args, **kwargs):\n",
        "        super(ResnetDT, self).__init__(*args, **kwargs)\n",
        "        self.temperature = nn.Parameter(torch.tensor(random.uniform(8.0, 15.0)))\n",
        "\n",
        "        if init_xavier == True:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, nn.Linear):\n",
        "                    nn.init.xavier_normal_(m.weight, gain = 1)\n",
        "\n",
        "    def get_temperature(self):\n",
        "        # Ensure temperature stays positive\n",
        "        return F.softplus(self.temperature)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        # x = torch.div(self.fc(x), self.t_student)\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "class TeacherTemperatureScheduler:\n",
        "    def __init__(\n",
        "        self,\n",
        "        initial_temp,\n",
        "        patience=5,\n",
        "        temp_min=1.0,\n",
        "        temp_max=6.0,\n",
        "        temp_step=1.0,\n",
        "        improvement_threshold=1e-3\n",
        "    ):\n",
        "        self.temperature = initial_temp\n",
        "        self.patience = patience\n",
        "        self.temp_min = temp_min\n",
        "        self.temp_max = temp_max\n",
        "        self.temp_step = temp_step\n",
        "        self.improvement_threshold = improvement_threshold\n",
        "\n",
        "        # Initialize tracking variables\n",
        "        self.best_loss = float('inf')\n",
        "        self.waiting = 0\n",
        "        self.loss_history = []\n",
        "\n",
        "    def step(self, current_loss):\n",
        "        \"\"\"\n",
        "        Update temperature based on loss trend\n",
        "        Returns: bool indicating if temperature was adjusted\n",
        "        \"\"\"\n",
        "        self.loss_history.append(current_loss)\n",
        "        temp_adjusted = False\n",
        "\n",
        "        # Need at least patience + 1 losses to make a decision\n",
        "        if len(self.loss_history) > self.patience:\n",
        "            # Get the trend of last patience+1 losses\n",
        "            recent_losses = self.loss_history[-self.patience-1:]\n",
        "            loss_improved = recent_losses[-1] < (min(recent_losses[:-1]) - self.improvement_threshold)\n",
        "            loss_worsened = recent_losses[-1] > (min(recent_losses[:-1]) + self.improvement_threshold)\n",
        "\n",
        "            if loss_improved:\n",
        "                # Reset waiting counter if loss improved significantly\n",
        "                self.waiting = 0\n",
        "                if current_loss < self.best_loss:\n",
        "                    self.best_loss = current_loss\n",
        "            else:\n",
        "                self.waiting += 1\n",
        "\n",
        "            # Adjust temperature if waited enough\n",
        "            if self.waiting >= self.patience:\n",
        "                if loss_worsened:\n",
        "                    # If loss is getting worse, increase temperature\n",
        "                    self.temperature = min(\n",
        "                        self.temp_max,\n",
        "                        self.temperature + self.temp_step\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    # If loss plateaued or slightly worse, decrease temperature\n",
        "                    self.temperature = max(\n",
        "                        self.temp_min,\n",
        "                        self.temperature - self.temp_step\n",
        "                    )\n",
        "                self.waiting = 0\n",
        "                temp_adjusted = True\n",
        "\n",
        "        return temp_adjusted\n",
        "\n",
        "class DistillationTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        student_model,\n",
        "        teacher_model,\n",
        "        alpha=0.01,\n",
        "        scheduler_patience=5,\n",
        "        scheduler_min_temp=1.0,\n",
        "        scheduler_max_temp=6.0\n",
        "    ):\n",
        "        self.student = student_model\n",
        "        self.teacher = teacher_model\n",
        "        self.teacher_temperature = random.uniform(4.0, 6.0)\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # Initialize temperature scheduler\n",
        "        self.temp_scheduler = TeacherTemperatureScheduler(\n",
        "            initial_temp=self.teacher_temperature,\n",
        "            patience=scheduler_patience,\n",
        "            temp_min=scheduler_min_temp,\n",
        "            temp_max=scheduler_max_temp\n",
        "        )\n",
        "\n",
        "        print(f\"Initial Student Temperature: {self.student.get_temperature().item():.2f}\")\n",
        "        print(f\"Initial Teacher Temperature: {self.teacher_temperature:.2f}\")\n",
        "\n",
        "    def accuracy(self, output, target, topk=(1,)):\n",
        "        \"\"\"\n",
        "        Computes the accuracy over the top-k predictions for the specified values of k.\n",
        "\n",
        "        Args:\n",
        "            output (torch.Tensor): Logits or predicted outputs from the model (batch_size x num_classes).\n",
        "            target (torch.Tensor): True labels (batch_size).\n",
        "            topk (tuple of int): Specifies the top-k accuracies to be calculated.\n",
        "\n",
        "        Returns:\n",
        "            list: List of accuracies corresponding to each value in topk.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            maxk = max(topk)  # Maximum k value, e.g., top-5 if topk=(1,5)\n",
        "            batch_size = target.size(0)\n",
        "\n",
        "            # Get the top-k indices with highest probability/logits\n",
        "            _, pred = output.topk(maxk, dim=1, largest=True, sorted=True)\n",
        "\n",
        "            # Convert to shape (k, batch_size) and check if predictions are correct\n",
        "            pred = pred.t()\n",
        "            correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "            # Calculate accuracy for each k in topk\n",
        "            res = []\n",
        "            for k in topk:\n",
        "                correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "                res.append(correct_k.mul_(100.0 / batch_size).item())\n",
        "\n",
        "            return res\n",
        "    def compute_loss(self, student_logits, teacher_logits, targets):\n",
        "        student_temp = self.student.get_temperature()\n",
        "        teacher_temp = self.teacher_temperature\n",
        "\n",
        "        # Standard cross-entropy with hard targets\n",
        "        hard_loss = F.cross_entropy(student_logits, targets)\n",
        "\n",
        "        # Soft targets from teacher\n",
        "        soft_targets = F.softmax(teacher_logits / teacher_temp, dim=1)\n",
        "\n",
        "        # Student predictions with learned temperature\n",
        "        soft_pred = F.log_softmax(student_logits / student_temp, dim=1)\n",
        "\n",
        "        # KL divergence loss scaled by temperatures\n",
        "        distill_loss = F.kl_div(\n",
        "            soft_pred,\n",
        "            soft_targets,\n",
        "            reduction='batchmean'\n",
        "        ) * (student_temp * teacher_temp)\n",
        "\n",
        "        # Combined loss\n",
        "        total_loss = (1 - self.alpha) * hard_loss + self.alpha * distill_loss\n",
        "        return total_loss, hard_loss, distill_loss\n",
        "\n",
        "    def train(self, train_loader, val_loader, epochs, learning_rate=1e-3):\n",
        "        epoch_acc1 = 0.0\n",
        "        epoch_acc5 = 0.0\n",
        "\n",
        "        val_acc1 = 0.0\n",
        "        val_acc5 = 0.0\n",
        "\n",
        "        optimizer = torch.optim.SGD([\n",
        "            {'params': [p for n, p in self.student.named_parameters() if 'temperature' not in n]},\n",
        "            {'params': self.student.temperature, 'lr': learning_rate * 0.1}\n",
        "          ],\n",
        "          lr=learning_rate,\n",
        "          momentum=0.9,\n",
        "          weight_decay=0.0001\n",
        "      )\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            self.student.train()\n",
        "            train_loss = 0.0\n",
        "            for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "                data = data.to('cuda')\n",
        "                targets = targets.to('cuda')\n",
        "\n",
        "                student_logits = self.student(data)\n",
        "                with torch.no_grad():\n",
        "                    teacher_logits = self.teacher(data)\n",
        "\n",
        "                total_loss, hard_loss, distill_loss = self.compute_loss(\n",
        "                    student_logits, teacher_logits, targets\n",
        "                )\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    acc1, acc5 = self.accuracy(student_logits, targets, (1, 5))\n",
        "\n",
        "                # Update running metrics\n",
        "                batch_size = data.size(0)\n",
        "                epoch_acc1 += acc1 * batch_size\n",
        "                epoch_acc5 += acc5 * batch_size\n",
        "\n",
        "                train_loss += total_loss.item()\n",
        "\n",
        "                if batch_idx % 100 == 0:\n",
        "                    print(f'Epoch {epoch}, Batch {batch_idx}:')\n",
        "                    print(f'  Loss: {total_loss.item():.4f}')\n",
        "                    print(f'  Student Temp: {self.student.get_temperature().item():.2f}')\n",
        "                    print(f'  Teacher Temp: {self.teacher_temperature:.2f}')\n",
        "                    print(f'  Accuracy@1: {acc1:.2f}%, Accuracy@5: {acc5:.2f}%\\n')\n",
        "                    print(f'  logits v target: { student_logits} v {targets}')\n",
        "\n",
        "            avg_train_loss = train_loss / len(train_loader)\n",
        "            epoch_acc1 /= len(train_loader.dataset)\n",
        "            epoch_acc5 /= len(train_loader.dataset)\n",
        "\n",
        "            # Validation phase\n",
        "            self.student.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for data, targets in val_loader:\n",
        "                    student_logits = self.student(data)\n",
        "                    teacher_logits = self.teacher(data)\n",
        "                    total_loss, _, _ = self.compute_loss(\n",
        "                        student_logits, teacher_logits, targets\n",
        "                    )\n",
        "                    val_loss += total_loss.item()\n",
        "\n",
        "                    acc1, acc5 = self.accuracy(student_logits, targets, (1, 5))\n",
        "                    val_acc1 += acc1 * data.size(0)\n",
        "                    val_acc5 += acc5 * data.size(0)\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            val_acc1 /= len(val_loader.dataset)\n",
        "            val_acc5 /= len(val_loader.dataset)\n",
        "\n",
        "            # Update teacher temperature using scheduler\n",
        "            temp_adjusted = self.temp_scheduler.step(avg_val_loss)\n",
        "            if temp_adjusted:\n",
        "                self.teacher_temperature = self.temp_scheduler.temperature\n",
        "\n",
        "            # Print epoch summary\n",
        "            print(f'\\nEpoch {epoch} Summary:')\n",
        "            print(f'  Train Loss: {avg_train_loss:.4f}')\n",
        "            print(f'  Val Loss: {avg_val_loss:.4f}')\n",
        "            print(f'  Student Temperature: {self.student.get_temperature().item():.2f}')\n",
        "            print(f'  Teacher Temperature: {self.teacher_temperature:.2f}')\n",
        "            print(f'  Val Accuracy@1: {val_acc1:.2f}%')\n",
        "            print(f'  Val Accuracy@5: {val_acc5:.2f}%\\n')\n",
        "            if temp_adjusted:\n",
        "                print('  Teacher temperature adjusted!')\n",
        "            print(f'  Scheduler waiting counter: {self.temp_scheduler.waiting}\\n')"
      ],
      "metadata": {
        "id": "M5WfbOWrBvih"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_model = ResnetDT(block_type=\"BasicBlock\", init_xavier=True, block=BasicBlock, layers=[3, 4, 6, 3], num_classes=1000)"
      ],
      "metadata": {
        "id": "-8p9HrNZLs6H"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = DistillationTrainer(student_model.to('cuda'), teacher_model)\n",
        "\n",
        "# Train\n",
        "# train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, generator=torch.Generator(device='cuda'))\n",
        "# val_loader = DataLoader(val_dataset, batch_size=256, shuffle=True, generator=torch.Generator(device='cuda'))\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "trainer.train(train_loader, val_loader, epochs=10)"
      ],
      "metadata": {
        "id": "U9cWi1LIKNvo",
        "outputId": "9b6cdac5-ee63-4dde-9bc7-4ecf8f87149e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Student Temperature: 11.68\n",
            "Initial Teacher Temperature: 5.96\n",
            "Epoch 0, Batch 0:\n",
            "  Loss: 4.2670\n",
            "  Student Temp: 11.68\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "Epoch 0, Batch 100:\n",
            "  Loss: 4.0387\n",
            "  Student Temp: 11.68\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "\n",
            "Epoch 0 Summary:\n",
            "  Train Loss: 4.1071\n",
            "  Val Loss: 4.0387\n",
            "  Student Temperature: 11.68\n",
            "  Teacher Temperature: 5.96\n",
            "  Val Accuracy@1: 0.15%\n",
            "  Val Accuracy@5: 0.74%\n",
            "\n",
            "  Scheduler waiting counter: 0\n",
            "\n",
            "Epoch 1, Batch 0:\n",
            "  Loss: 4.0035\n",
            "  Student Temp: 11.68\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "Epoch 1, Batch 100:\n",
            "  Loss: 3.9166\n",
            "  Student Temp: 11.67\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "\n",
            "Epoch 1 Summary:\n",
            "  Train Loss: 3.9538\n",
            "  Val Loss: 3.9262\n",
            "  Student Temperature: 11.67\n",
            "  Teacher Temperature: 5.96\n",
            "  Val Accuracy@1: 0.15%\n",
            "  Val Accuracy@5: 0.64%\n",
            "\n",
            "  Scheduler waiting counter: 0\n",
            "\n",
            "Epoch 2, Batch 0:\n",
            "  Loss: 3.8817\n",
            "  Student Temp: 11.67\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "Epoch 2, Batch 100:\n",
            "  Loss: 3.8963\n",
            "  Student Temp: 11.67\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "\n",
            "Epoch 2 Summary:\n",
            "  Train Loss: 3.9042\n",
            "  Val Loss: 3.9035\n",
            "  Student Temperature: 11.67\n",
            "  Teacher Temperature: 5.96\n",
            "  Val Accuracy@1: 0.18%\n",
            "  Val Accuracy@5: 0.69%\n",
            "\n",
            "  Scheduler waiting counter: 0\n",
            "\n",
            "Epoch 3, Batch 0:\n",
            "  Loss: 3.8446\n",
            "  Student Temp: 11.67\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "Epoch 3, Batch 100:\n",
            "  Loss: 3.8810\n",
            "  Student Temp: 11.66\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "\n",
            "Epoch 3 Summary:\n",
            "  Train Loss: 3.8826\n",
            "  Val Loss: 3.8909\n",
            "  Student Temperature: 11.66\n",
            "  Teacher Temperature: 5.96\n",
            "  Val Accuracy@1: 0.05%\n",
            "  Val Accuracy@5: 0.94%\n",
            "\n",
            "  Scheduler waiting counter: 0\n",
            "\n",
            "Epoch 4, Batch 0:\n",
            "  Loss: 3.8211\n",
            "  Student Temp: 11.66\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "Epoch 4, Batch 100:\n",
            "  Loss: 3.8745\n",
            "  Student Temp: 11.66\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "\n",
            "Epoch 4 Summary:\n",
            "  Train Loss: 3.8691\n",
            "  Val Loss: 3.8834\n",
            "  Student Temperature: 11.66\n",
            "  Teacher Temperature: 5.96\n",
            "  Val Accuracy@1: 0.13%\n",
            "  Val Accuracy@5: 0.82%\n",
            "\n",
            "  Scheduler waiting counter: 0\n",
            "\n",
            "Epoch 5, Batch 0:\n",
            "  Loss: 3.8044\n",
            "  Student Temp: 11.66\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "Epoch 5, Batch 100:\n",
            "  Loss: 3.8659\n",
            "  Student Temp: 11.66\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "\n",
            "Epoch 5 Summary:\n",
            "  Train Loss: 3.8603\n",
            "  Val Loss: 3.8824\n",
            "  Student Temperature: 11.65\n",
            "  Teacher Temperature: 5.96\n",
            "  Val Accuracy@1: 0.18%\n",
            "  Val Accuracy@5: 1.10%\n",
            "\n",
            "  Scheduler waiting counter: 1\n",
            "\n",
            "Epoch 6, Batch 0:\n",
            "  Loss: 3.7911\n",
            "  Student Temp: 11.65\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "Epoch 6, Batch 100:\n",
            "  Loss: 3.8593\n",
            "  Student Temp: 11.65\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "\n",
            "Epoch 6 Summary:\n",
            "  Train Loss: 3.8531\n",
            "  Val Loss: 3.8770\n",
            "  Student Temperature: 11.65\n",
            "  Teacher Temperature: 5.96\n",
            "  Val Accuracy@1: 0.15%\n",
            "  Val Accuracy@5: 1.10%\n",
            "\n",
            "  Scheduler waiting counter: 0\n",
            "\n",
            "Epoch 7, Batch 0:\n",
            "  Loss: 3.7815\n",
            "  Student Temp: 11.65\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "Epoch 7, Batch 100:\n",
            "  Loss: 3.8523\n",
            "  Student Temp: 11.65\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "\n",
            "Epoch 7 Summary:\n",
            "  Train Loss: 3.8466\n",
            "  Val Loss: 3.8746\n",
            "  Student Temperature: 11.64\n",
            "  Teacher Temperature: 5.96\n",
            "  Val Accuracy@1: 0.13%\n",
            "  Val Accuracy@5: 0.99%\n",
            "\n",
            "  Scheduler waiting counter: 0\n",
            "\n",
            "Epoch 8, Batch 0:\n",
            "  Loss: 3.7750\n",
            "  Student Temp: 11.64\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "Epoch 8, Batch 100:\n",
            "  Loss: 3.8476\n",
            "  Student Temp: 11.64\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "\n",
            "Epoch 8 Summary:\n",
            "  Train Loss: 3.8405\n",
            "  Val Loss: 3.8736\n",
            "  Student Temperature: 11.64\n",
            "  Teacher Temperature: 5.96\n",
            "  Val Accuracy@1: 0.20%\n",
            "  Val Accuracy@5: 0.92%\n",
            "\n",
            "  Scheduler waiting counter: 0\n",
            "\n",
            "Epoch 9, Batch 0:\n",
            "  Loss: 3.7665\n",
            "  Student Temp: 11.64\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "Epoch 9, Batch 100:\n",
            "  Loss: 3.8407\n",
            "  Student Temp: 11.64\n",
            "  Teacher Temp: 5.96\n",
            "  Accuracy@1: 0.00%\n",
            "  Accuracy@5: 0.00%\n",
            "\n",
            "\n",
            "Epoch 9 Summary:\n",
            "  Train Loss: 3.8349\n",
            "  Val Loss: 3.8717\n",
            "  Student Temperature: 11.64\n",
            "  Teacher Temperature: 5.96\n",
            "  Val Accuracy@1: 0.18%\n",
            "  Val Accuracy@5: 1.10%\n",
            "\n",
            "  Scheduler waiting counter: 0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-27T12:18:31.068163Z",
          "iopub.execute_input": "2024-10-27T12:18:31.068487Z",
          "iopub.status.idle": "2024-10-27T12:18:31.075908Z",
          "shell.execute_reply.started": "2024-10-27T12:18:31.068454Z",
          "shell.execute_reply": "2024-10-27T12:18:31.074541Z"
        },
        "trusted": true,
        "id": "cZH3N7Q6V10F"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-27T12:18:31.078085Z",
          "iopub.execute_input": "2024-10-27T12:18:31.078773Z",
          "iopub.status.idle": "2024-10-27T12:18:31.090851Z",
          "shell.execute_reply.started": "2024-10-27T12:18:31.078729Z",
          "shell.execute_reply": "2024-10-27T12:18:31.089638Z"
        },
        "trusted": true,
        "id": "XLb5557PV10G",
        "outputId": "de9611f1-8d2d-47c2-efca-deab06eff924",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 6            |        cudaMalloc retries: 7         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |   6783 MiB |  14820 MiB |  79762 MiB |  72979 MiB |\\n|       from large pool |   6725 MiB |  14748 MiB |  78724 MiB |  71999 MiB |\\n|       from small pool |     57 MiB |    219 MiB |   1037 MiB |    979 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |   6783 MiB |  14820 MiB |  79762 MiB |  72979 MiB |\\n|       from large pool |   6725 MiB |  14748 MiB |  78724 MiB |  71999 MiB |\\n|       from small pool |     57 MiB |    219 MiB |   1037 MiB |    979 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |   6778 MiB |  14812 MiB |  79737 MiB |  72959 MiB |\\n|       from large pool |   6721 MiB |  14740 MiB |  78701 MiB |  71980 MiB |\\n|       from small pool |     57 MiB |    219 MiB |   1035 MiB |    978 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  10540 MiB |  14934 MiB |  15106 MiB |   4566 MiB |\\n|       from large pool |  10472 MiB |  14860 MiB |  14860 MiB |   4388 MiB |\\n|       from small pool |     68 MiB |    246 MiB |    246 MiB |    178 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |   3756 MiB |   4548 MiB |  59497 MiB |  55740 MiB |\\n|       from large pool |   3746 MiB |   4534 MiB |  58182 MiB |  54436 MiB |\\n|       from small pool |     10 MiB |     26 MiB |   1314 MiB |   1304 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     921    |    1693    |    6728    |    5807    |\\n|       from large pool |      93    |     203    |     491    |     398    |\\n|       from small pool |     828    |    1528    |    6237    |    5409    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     921    |    1693    |    6728    |    5807    |\\n|       from large pool |      93    |     203    |     491    |     398    |\\n|       from small pool |     828    |    1528    |    6237    |    5409    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      61    |     162    |     164    |     103    |\\n|       from large pool |      27    |      41    |      41    |      14    |\\n|       from small pool |      34    |     123    |     123    |      89    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      35    |     103    |    2200    |    2165    |\\n|       from large pool |      25    |      28    |     200    |     175    |\\n|       from small pool |      10    |      92    |    2000    |    1990    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}